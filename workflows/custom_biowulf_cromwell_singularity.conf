# include the application.conf at the top
include required(classpath("application"))

system {
  job-rate-control {
    jobs = 1
    per = 1 second
  }
}

system.io {
  number-of-attempts = 5
}

backend {
  default = "Slurm"
  providers {
    Slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory" 
      config {
        concurrent-job-limit = 50
        script-epilogue = ""
        runtime-attributes = """
        Int maxRetries = 5
        Int time = 600
        Int cpu = 2
        Float memory_gb = 20.0
        String docker
        String? disks = "local-disk 20 SSD"
        String? mount1 = ""
        String? mount2 = ""
        """

        submit = """
            sbatch \
              --partition=norm \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${time} \
              ${"-c " + cpu} \
              --mem=$(echo $(printf %.0f "${memory_gb}")"g") \
              --gres=lscratch:$(echo "${disks}" | cut -f 2 -d " ") \
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """
            bind_directories_subcmd="--bind ${cwd}:${docker_cwd}"
            new_script=$(echo "${script}" | sed 's/.*\(\/cromwell-executions\)/\1/g')
            if [ ! "${mount1}" = "" ]; then
               directory_basename=$(basename ${mount1})
               bind_directories_subcmd="$bind_directories_subcmd,${mount1}:${docker_cwd}/execution/$directory_basename"
            fi
            if [ ! "${mount2}" = "" ]; then
               directory_basename=$(basename ${mount2})
               bind_directories_subcmd="$bind_directories_subcmd,${mount2}:${docker_cwd}/execution/$directory_basename"
            fi
            echo $bind_directories_subcmd
            echo ${docker_cwd}
            
            # Ensure singularity is loaded if it's installed as a module
            module load singularity

            # Build the Docker image into a singularity image
            DOCKER_CONTAINER_DIR="/data/$USER/containers"
            if [ ! -d $DOCKER_CONTAINER_DIR ]; then
                mkdir -p $DOCKER_CONTAINER_DIR
                chmod 2770 $DOCKER_CONTAINER_DIR
            fi
            DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
            IMAGE=$DOCKER_CONTAINER_DIR/$DOCKER_NAME.sif
            if [ ! -f $IMAGE ]; then
                singularity pull $IMAGE docker://${docker}
            fi
            
            # Submit the script to SLURM
            sbatch \
              --partition=norm \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${time} \
              ${"-c " + cpu} \
              --mem=$(echo $(printf %.0f "${memory_gb}")"g") \
              --gres=lscratch:$(echo "${disks}" | cut -f 2 -d " ") \
              --wrap "module load singularity; export SINGULARITY_CACHEDIR=/data/markellocj/singularity_cache/; time singularity exec $bind_directories_subcmd $IMAGE ${job_shell} $new_script"
        """
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "(\\d+).*"
      }
    }
  }
}


